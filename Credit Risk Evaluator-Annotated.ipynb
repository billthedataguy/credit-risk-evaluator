{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 19\n",
    "## William Vann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Prediction: \n",
    "\n",
    "My prediction is that **Logistic Regression (LR) will perform better than Random Forest Classification (RFC)**. \n",
    "\n",
    "One reason is that the credit risk data problem is similar to other problems (like medical disease risk) we've used LR effectively for. We are looking for a prediction of 0 or 1 for credit risk, not a continuous numeric value, and LR is a standard and trusty choice (often the first choice, at least for testing) for such situations. \n",
    "\n",
    "Of course RFC is a good choice for such situations as well, so I've considered what I know about the math underlying each approach to favor LR.  My understanding is that the math underlying LR involves calculation of probabilities, while the math underlying RFC involves a \"majority voting\" scenario among some number of decision trees.  This particular data problem seems more probabilistic to me, as the question \"Is person A a good credit risk (loan_status=0), or a bad credit risk (loan_status=1)?\" seems better answered with \"Yes (with an x% probability\" than \"Yes (a majority of the trees vote Yes)\". I am persuaded on this point even more when I consider that LR admits of a \"threshold\" probability that the data scientist can change, so as a result it seems like it might be more \"fine-tunable\" than RFC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "\n",
    "df = pd.read_csv(\"Resources/lending_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"loan_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"loan_status\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"loan_status\"]).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"loan_status\", axis=1).values\n",
    "\n",
    "y = df[\"loan_status\"].values\n",
    "\n",
    "target_names=[\"good risk\", \"bad risk\"] # loan_status=0 or 1, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler to standardize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Train the scaler with the X_train data.\n",
    "\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train and X_test.\n",
    "# Note that the scaler used to transform X_train and X_test was trained on X_train.\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create predictions with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model \n",
    "\n",
    "model = LogisticRegression().fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the model\n",
    "\n",
    "print(f\"Logisitic Regression Training Data Score: {model.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Logisitic Regression Testing Data Score: {model.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "cm_lr = confusion_matrix(y_true, y_pred)\n",
    "cr_lr = classification_report(y_true, y_pred, target_names=target_names)\n",
    "\n",
    "print(f\"Logisitic Regression CONFUSION MATRIX: \\n\\n {cm_lr}\\n\\n\")\n",
    "print(f\"Logisitic Regression CLASSIFICATION REPORT: \\n\\n {cr_lr}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create predictions with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier model\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=500).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the model\n",
    "\n",
    "print(f\"Random Forest Classifier Training Score: {clf.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Random Forest Classifier Testing Score: {clf.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "cm_rfc = confusion_matrix(y_true, y_pred)\n",
    "cr_rfc = classification_report(y_true, y_pred, target_names=target_names)\n",
    "\n",
    "print(f\"Random Forest Classifier CONFUSION MATRIX: \\n\\n {cm_rfc}\\n\\n\")\n",
    "print(f\"Random Forest Classifier CLASSIFICATION REPORT: \\n\\n {cr_rfc}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Reflection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LR: {cr_lr}\")\n",
    "print(f\"RFC: {cr_rfc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression is the Better-Performing Model\n",
    "\n",
    "Looking at the classification reports for the two models, both models perform well on precision, recall, and f1-score for \"good risk\", but less well for the same metrics for \"bad risk\". Nevertheless, LR performed better than RFC on \"bad risk\" recall and f1-score. Therefore, my prediction was confirmed.  \n",
    "\n",
    "After exploring the dataset, it appears that the dataset is somewhat \"lop-sided\" in that only about 3% of the sample had a \"loan status\" value of 1 (i.e., meaning loan was not approved, they were judged to be a \"bad\" credit risk).  Hence, 97% of the sample was approved for a loan, judged to be a \"good\" risk.  My intuition is that we would expect most of the classification models to perform well with such a skew in the training dataset, and that was borne out in our comparison of two of the most-often used binary classification models.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
