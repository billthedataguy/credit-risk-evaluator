{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 19\n",
    "## William Vann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Prediction: \n",
    "\n",
    "My prediction is that **Logistic Regression (LR) will perform better than Random Forest Classification (RFC)**. \n",
    "\n",
    "One reason is that the credit risk data problem is similar to other problems (like medical disease risk) we've used LR effectively for. We are looking for a prediction of 0 or 1 for credit risk, not a continuous numeric value, and LR is a standard and trusty choice (often the first choice, at least for testing) for such situations. \n",
    "\n",
    "Of course RFC is a good choice for such situations as well, so I've considered what I know about the math underlying each approach to favor LR.  My understanding is that the math underlying LR involves calculation of probabilities, while the math underlying RFC involves a \"majority voting\" scenario among some number of decision trees.  This particular data problem seems more probabilistic to me, as the question \"Is person A a good credit risk?\" seems better answered with \"Yes (with an x% probability\" than \"Yes (a majority of the trees vote Yes)\". I am persuaded on this point even more when I consider that LR admits of a \"threshold\" probability that the data scientist can change, so as a result it seems like it might be more \"fine-tunable\" than RFC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "\n",
    "df = pd.read_csv(\"Resources/lending_data.csv\", encoding=\"utf-8\", low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"loan_status\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"loan_status\"]).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"loan_status\", axis=1).values\n",
    "\n",
    "y = df[\"loan_status\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler to standardize the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Train the scaler with the X_train data.\n",
    "\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train and X_test.\n",
    "# Note that the scaler used to transform X_train and X_test was trained on X_train.\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create predictions with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression().fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the model\n",
    "\n",
    "print(f\"Logisitic Regression Training Data Score: {model.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Logisitic Regression Testing Data Score: {model.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "cm_lr = confusion_matrix(y_true, y_pred)\n",
    "cr_lr = classification_report(y_true, y_pred)\n",
    "\n",
    "print(f\"Logisitic Regression CONFUSION MATRIX: \\n\\n {cm_lr}\\n\\n\")\n",
    "print(f\"Logisitic Regression CLASSIFICATION REPORT: \\n\\n {cr_lr}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create predictions with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=500).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the model\n",
    "\n",
    "print(f\"Random Forest Classifier Training Score: {clf.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Random Forest Classifier Testing Score: {clf.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_true = y_test\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "cm_rfc = confusion_matrix(y_true, y_pred)\n",
    "cr_rfc = classification_report(y_true, y_pred)\n",
    "\n",
    "print(f\"Random Forest Classifier CONFUSION MATRIX: \\n\\n {cm_rfc}\\n\\n\")\n",
    "print(f\"Random Forest Classifier CLASSIFICATION REPORT: \\n\\n {cr_rfc}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Reflection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LR: {cr_lr}\")\n",
    "print(f\"RFC: {cr_rfc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression is the Better-Performing Model\n",
    "\n",
    "Looking at the classification reports for the two models, it is clear that LR wins on all metrics except for macro average on precision (where there is only a 0.01 difference in favor of RFC). Both models perform well on precision, recall, and f1-score for loan denial, but less well for the same metrics for loan approval. Nevertheless, LR performed substantially better than RFC on recall and f1-score on loan approval. Therefore, my prediction was confirmed.  \n",
    "\n",
    "After exploring the dataset, it appears that the dataset is somewhat \"lop-sided\" in that only about 3% of the sample had a \"loan status\" value of 1 (i.e., meaning loan was approved, they were judged to be a good credit risk).  Hence, 97% of the sample was not approved for a loan.  My intuition is that we would expect most of the classification models to perform well with such a skew in the training dataset, and that was borne out in our comparison of two of the most-often used binary classification models.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
